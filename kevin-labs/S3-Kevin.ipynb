{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# IHLT - Lab 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read all pairs of sentences of the trial set within the evaluation framework of the project.\n",
    "\n",
    "2. Compute their similarities by considering lemmas and Jaccard distance.\n",
    "\n",
    "3. Compare the results with those in session 2 (document structure) in which words were considered.\n",
    "\n",
    "4. Compare the results with gold standard by giving the pearson correlation between them.\n",
    "\n",
    "5. Questions (justify the answers):\n",
    "\n",
    "    - Which is better: words or lemmas?\n",
    "\n",
    "    - Do you think that could perform better for any pair of texts?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Data preparation\n",
    "\n",
    "As it was done in the last lab session, all pairs of sentences of the trial set are read and stored in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. ('The bird is bathing in the sink.', 'Birdie is washing itself in the water basin.')\n",
      "2. ('In May 2010, the troops attempted to invade Kabul.', 'The US army invaded Kabul on May 7th last year, 2010.')\n",
      "3. ('John said he is considered a witness but not a suspect.', '\"He is not a suspect anymore.\" John said.')\n",
      "4. ('They flew out of the nest in groups.', 'They flew into the nest together.')\n",
      "5. ('The woman is playing the violin.', 'The young lady enjoys listening to the guitar.')\n",
      "6. ('John went horse back riding at dawn with a whole group of friends.', 'Sunrise at dawn is a magnificent view to take in if you wake up early enough for it.')\n"
     ]
    }
   ],
   "source": [
    "pairs = list()\n",
    "with open('STS.input.txt','r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = nltk.TabTokenizer().tokenize(line.strip())\n",
    "        pairs.append((line[1], line[2]))\n",
    "        \n",
    "for index, pair in enumerate(pairs):\n",
    "    print(str(index + 1) + \".\", pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bird is bathing in the sink.\n"
     ]
    }
   ],
   "source": [
    "print(pairs[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, each sentence is separated by its words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. (['The', 'bird', 'is', 'bathing', 'in', 'the', 'sink', '.'], ['Birdie', 'is', 'washing', 'itself', 'in', 'the', 'water', 'basin', '.']) \n",
      "\n",
      "2. (['In', 'May', '2010', ',', 'the', 'troops', 'attempted', 'to', 'invade', 'Kabul', '.'], ['The', 'US', 'army', 'invaded', 'Kabul', 'on', 'May', '7th', 'last', 'year', ',', '2010', '.']) \n",
      "\n",
      "3. (['John', 'said', 'he', 'is', 'considered', 'a', 'witness', 'but', 'not', 'a', 'suspect', '.'], ['``', 'He', 'is', 'not', 'a', 'suspect', 'anymore', '.', \"''\", 'John', 'said', '.']) \n",
      "\n",
      "4. (['They', 'flew', 'out', 'of', 'the', 'nest', 'in', 'groups', '.'], ['They', 'flew', 'into', 'the', 'nest', 'together', '.']) \n",
      "\n",
      "5. (['The', 'woman', 'is', 'playing', 'the', 'violin', '.'], ['The', 'young', 'lady', 'enjoys', 'listening', 'to', 'the', 'guitar', '.']) \n",
      "\n",
      "6. (['John', 'went', 'horse', 'back', 'riding', 'at', 'dawn', 'with', 'a', 'whole', 'group', 'of', 'friends', '.'], ['Sunrise', 'at', 'dawn', 'is', 'a', 'magnificent', 'view', 'to', 'take', 'in', 'if', 'you', 'wake', 'up', 'early', 'enough', 'for', 'it', '.']) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pairs = [(nltk.word_tokenize(p[0]), nltk.word_tokenize(p[1])) for p in pairs]\n",
    "\n",
    "for index, pair in enumerate(pairs):\n",
    "    print(str(index + 1) + \".\", pair, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session, lemmas are considered instead of words, so they are computed using the class `WordNetLemmatizer` from `nltk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(p, lower=False):\n",
    "    try:\n",
    "        return wnl.lemmatize(p[0].lower(), pos=p[1][0].lower())\n",
    "    except:\n",
    "        if lower:\n",
    "            return p[0].lower()\n",
    "        return p[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. ([('The', 'DT'), ('bird', 'NN'), ('is', 'VBZ'), ('bathing', 'VBG'), ('in', 'IN'), ('the', 'DT'), ('sink', 'NN'), ('.', '.')], [('Birdie', 'NNP'), ('is', 'VBZ'), ('washing', 'VBG'), ('itself', 'PRP'), ('in', 'IN'), ('the', 'DT'), ('water', 'NN'), ('basin', 'NN'), ('.', '.')]) \n",
      "\n",
      "2. ([('In', 'IN'), ('May', 'NNP'), ('2010', 'CD'), (',', ','), ('the', 'DT'), ('troops', 'NNS'), ('attempted', 'VBD'), ('to', 'TO'), ('invade', 'VB'), ('Kabul', 'NNP'), ('.', '.')], [('The', 'DT'), ('US', 'NNP'), ('army', 'NN'), ('invaded', 'VBD'), ('Kabul', 'NNP'), ('on', 'IN'), ('May', 'NNP'), ('7th', 'CD'), ('last', 'JJ'), ('year', 'NN'), (',', ','), ('2010', 'CD'), ('.', '.')]) \n",
      "\n",
      "3. ([('John', 'NNP'), ('said', 'VBD'), ('he', 'PRP'), ('is', 'VBZ'), ('considered', 'VBN'), ('a', 'DT'), ('witness', 'NN'), ('but', 'CC'), ('not', 'RB'), ('a', 'DT'), ('suspect', 'NN'), ('.', '.')], [('``', '``'), ('He', 'PRP'), ('is', 'VBZ'), ('not', 'RB'), ('a', 'DT'), ('suspect', 'NN'), ('anymore', 'RB'), ('.', '.'), (\"''\", \"''\"), ('John', 'NNP'), ('said', 'VBD'), ('.', '.')]) \n",
      "\n",
      "4. ([('They', 'PRP'), ('flew', 'VBD'), ('out', 'IN'), ('of', 'IN'), ('the', 'DT'), ('nest', 'JJS'), ('in', 'IN'), ('groups', 'NNS'), ('.', '.')], [('They', 'PRP'), ('flew', 'VBD'), ('into', 'IN'), ('the', 'DT'), ('nest', 'JJS'), ('together', 'RB'), ('.', '.')]) \n",
      "\n",
      "5. ([('The', 'DT'), ('woman', 'NN'), ('is', 'VBZ'), ('playing', 'VBG'), ('the', 'DT'), ('violin', 'NN'), ('.', '.')], [('The', 'DT'), ('young', 'JJ'), ('lady', 'NN'), ('enjoys', 'VBZ'), ('listening', 'VBG'), ('to', 'TO'), ('the', 'DT'), ('guitar', 'NN'), ('.', '.')]) \n",
      "\n",
      "6. ([('John', 'NNP'), ('went', 'VBD'), ('horse', 'NN'), ('back', 'RB'), ('riding', 'VBG'), ('at', 'IN'), ('dawn', 'NN'), ('with', 'IN'), ('a', 'DT'), ('whole', 'JJ'), ('group', 'NN'), ('of', 'IN'), ('friends', 'NNS'), ('.', '.')], [('Sunrise', 'NN'), ('at', 'IN'), ('dawn', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('magnificent', 'JJ'), ('view', 'NN'), ('to', 'TO'), ('take', 'VB'), ('in', 'IN'), ('if', 'IN'), ('you', 'PRP'), ('wake', 'VBP'), ('up', 'RP'), ('early', 'RB'), ('enough', 'RB'), ('for', 'IN'), ('it', 'PRP'), ('.', '.')]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pairs = [(pos_tag(p[0]), pos_tag(p[1])) for p in pairs]\n",
    "\n",
    "for index, pair in enumerate(pairs):\n",
    "    print(str(index + 1) + \".\", pair, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. (['The', 'bird', 'be', 'bath', 'in', 'the', 'sink', '.'], ['birdie', 'be', 'wash', 'itself', 'in', 'the', 'water', 'basin', '.']) \n",
      "\n",
      "2. (['In', 'may', '2010', ',', 'the', 'troop', 'attempt', 'to', 'invade', 'kabul', '.'], ['The', 'u', 'army', 'invade', 'kabul', 'on', 'may', '7th', 'last', 'year', ',', '2010', '.']) \n",
      "\n",
      "3. (['john', 'say', 'he', 'be', 'consider', 'a', 'witness', 'but', 'not', 'a', 'suspect', '.'], ['``', 'He', 'be', 'not', 'a', 'suspect', 'anymore', '.', \"''\", 'john', 'say', '.']) \n",
      "\n",
      "4. (['They', 'fly', 'out', 'of', 'the', 'nest', 'in', 'group', '.'], ['They', 'fly', 'into', 'the', 'nest', 'together', '.']) \n",
      "\n",
      "5. (['The', 'woman', 'be', 'play', 'the', 'violin', '.'], ['The', 'young', 'lady', 'enjoy', 'listen', 'to', 'the', 'guitar', '.']) \n",
      "\n",
      "6. (['john', 'go', 'horse', 'back', 'rid', 'at', 'dawn', 'with', 'a', 'whole', 'group', 'of', 'friend', '.'], ['sunrise', 'at', 'dawn', 'be', 'a', 'magnificent', 'view', 'to', 'take', 'in', 'if', 'you', 'wake', 'up', 'early', 'enough', 'for', 'it', '.']) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sw = set(stopwords.words('english'))\n",
    "\n",
    "t_pairs = list()\n",
    "t_pairs_lower = list()\n",
    "t_pairs_lower_no_sw = list()\n",
    "t_pairs_lower_ow = list()\n",
    "\n",
    "for pair in pairs:\n",
    "    t_pairs.append(([lemmatize(word) for word in pair[0]], [lemmatize(word) for word in pair[1]]))\n",
    "    t_pairs_lower.append(([lemmatize(word, True) for word in pair[0]],\n",
    "                          [lemmatize(word, True) for word in pair[1]]))\n",
    "    t_pairs_lower_no_sw.append(([lemmatize(word, True) for word in pair[0] if word[0].lower() not in sw],\n",
    "                                [lemmatize(word, True) for word in pair[1] if word[0].lower() not in sw]))\n",
    "    t_pairs_lower_ow.append(([lemmatize(word, True) for word in pair[0] if re.search(r\"\\w\", word[0])],\n",
    "                             [lemmatize(word, True) for word in pair[1] if re.search(r\"\\w\", word[0])]))\n",
    "    \n",
    "for index, t_pair in enumerate(t_pairs):\n",
    "    print(str(index + 1) + \".\", t_pair, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Similarities computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After tokenizing the sentences using lemmas, the similarities can be computed using the same steps of the last laboratory session. For that reason, [*Jaccard distance*](https://www.nltk.org/api/nltk.metrics.html#nltk.metrics.distance.jaccard_distance) is used in order to get the similarity between a pair of sentences.\n",
    "\n",
    "$ Similarity = 1 - Jaccard_{Distance} $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities (considering lemmas):\n",
      "\n",
      "1. 0.3076923076923077\n",
      "2. 0.33333333333333337\n",
      "3. 0.4666666666666667\n",
      "4. 0.4545454545454546\n",
      "5. 0.23076923076923073\n",
      "6. 0.13793103448275867\n"
     ]
    }
   ],
   "source": [
    "similarities_l = [1 - jaccard_distance(set(p[0]), set(p[1])) for p in t_pairs]\n",
    "similarities_l_lower = [1 - jaccard_distance(set(p[0]), set(p[1])) for p in t_pairs_lower]\n",
    "similarities_l_lower_no_sw = [1 - jaccard_distance(set(p[0]), set(p[1])) for p in t_pairs_lower_no_sw]\n",
    "similarities_l_lower_ow = [1 - jaccard_distance(set(p[0]), set(p[1])) for p in t_pairs_lower_ow]\n",
    "\n",
    "print(\"Similarities (considering lemmas):\\n\")\n",
    "for index, similarity in enumerate(similarities_l):\n",
    "    print(str(index + 1) + \".\", similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Similarities comparison with Session 2\n",
    "\n",
    "If each similarity is computed using the *Jaccard distance* of pairs tokenized by words (Session 2), the following results are obtained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities (considering words):\n",
      "\n",
      "1. 0.3076923076923077\n",
      "2. 0.26315789473684215\n",
      "3. 0.4666666666666667\n",
      "4. 0.4545454545454546\n",
      "5. 0.23076923076923073\n",
      "6. 0.13793103448275867\n"
     ]
    }
   ],
   "source": [
    "similarities_w = [1 - jaccard_distance(set(p[0]), set(p[1])) for p in pairs]\n",
    "\n",
    "print(\"Similarities (considering words):\\n\")\n",
    "for index, similarity in enumerate(similarities_w):\n",
    "    print(str(index + 1) + \".\", similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the results show, only the second similarity increases with the tokenization with lemmas (and improves since it should be a value near to 0.8/1.0 or 4.0/5.0).\n",
    "\n",
    "The reason behind this is that only in the second pair of sentences there is one more word that are in both sets in the *Jaccard distance* computation with lemmas:\n",
    "\n",
    "**Original Sentences**\n",
    "- 'In May 2010, the troops attempted to *invade* Kabul.'\n",
    "- 'The US army *invaded* Kabul on May 7th last year, 2010.'\n",
    "\n",
    "**Sentences tokenized by lemmas**\n",
    "- 'In may 2010, the troop attempt to **invade** kabul.'\n",
    "- 'The u army **invade** kabul on may 7th last year, 2010. '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Results comparison with gold standard\n",
    "Finally, as it was done in the last session, similarities are compared with the gold standard again to analyze the precision of the results and if they have improved or not.\n",
    "\n",
    "According to `00-readme.txt`, the similarity between two sentences that are completely equivalent should be 1/1 (or 5/5) and the similarity between two sentences that are on different topics should be 0/1 (or 0/5). For that reason, the reference similarities should be `[1.0, 0.8, 0.6, 0.4, 0.2, 0]` or its proportional values `[5, 4, 3, 2, 1, 0]`. \n",
    "\n",
    "The values in the gold standard file `STS.gs.txt` are reversed, so they will be read and then inverted in order to get them correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold standard: [5, 4, 3, 2, 1, 0]\n",
      "Pearson correlation (words): 0.3962389776119232\n",
      "Pearson correlation (lemmas): 0.490670810375692\n"
     ]
    }
   ],
   "source": [
    "gs = list()\n",
    "with open('trial/STS.gs.txt','r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = nltk.TabTokenizer().tokenize(line.strip())\n",
    "        gs.append(int(line[1]))\n",
    "\n",
    "gs.reverse()\n",
    "print(\"Gold standard:\", gs)\n",
    "print(\"Pearson correlation (words):\", pearsonr(gs, similarities_w)[0])\n",
    "print(\"Pearson correlation (lemmas):\", pearsonr(gs, similarities_l)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, [*Pearson correlation coefficients*](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html) of several types of words treatment before the lemmas tokenization are computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation (lemmas + lowercase): 0.5790860088205632\n",
      "Pearson correlation (lemmas + lowercase + no stopwords): 0.2294630424388053\n",
      "Pearson correlation (lemmas + lowercase + only words (no punctuation marks)): 0.47220783380206716\n"
     ]
    }
   ],
   "source": [
    "print(\"Pearson correlation (lemmas + lowercase):\", \n",
    "      pearsonr(gs, similarities_l_lower)[0])\n",
    "print(\"Pearson correlation (lemmas + lowercase + no stopwords):\", \n",
    "      pearsonr(gs, similarities_l_lower_no_sw)[0])\n",
    "print(\"Pearson correlation (lemmas + lowercase + only words (no punctuation marks)):\", \n",
    "      pearsonr(gs, similarities_l_lower_ow)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen, the preprocessing that fits better for this task in these examples is done by taking lowercase of pairs of sentences without removing stopwords and punctuation marks. Nevertheless, stopwords and punctuation tokens do not add any meaning, so the underlying reason of this decrement of correlation between obtained results and expected results is the presence of tokens like \".\" and \"the\" in both sentences in the first pair.\n",
    "\n",
    "These are the pairs of sentences in the lemmas + lowercase computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. (['the', 'bird', 'be', 'bath', 'in', 'the', 'sink', '.'], ['birdie', 'be', 'wash', 'itself', 'in', 'the', 'water', 'basin', '.']) \n",
      "\n",
      "2. (['in', 'may', '2010', ',', 'the', 'troop', 'attempt', 'to', 'invade', 'kabul', '.'], ['the', 'u', 'army', 'invade', 'kabul', 'on', 'may', '7th', 'last', 'year', ',', '2010', '.']) \n",
      "\n",
      "3. (['john', 'say', 'he', 'be', 'consider', 'a', 'witness', 'but', 'not', 'a', 'suspect', '.'], ['``', 'he', 'be', 'not', 'a', 'suspect', 'anymore', '.', \"''\", 'john', 'say', '.']) \n",
      "\n",
      "4. (['they', 'fly', 'out', 'of', 'the', 'nest', 'in', 'group', '.'], ['they', 'fly', 'into', 'the', 'nest', 'together', '.']) \n",
      "\n",
      "5. (['the', 'woman', 'be', 'play', 'the', 'violin', '.'], ['the', 'young', 'lady', 'enjoy', 'listen', 'to', 'the', 'guitar', '.']) \n",
      "\n",
      "6. (['john', 'go', 'horse', 'back', 'rid', 'at', 'dawn', 'with', 'a', 'whole', 'group', 'of', 'friend', '.'], ['sunrise', 'at', 'dawn', 'be', 'a', 'magnificent', 'view', 'to', 'take', 'in', 'if', 'you', 'wake', 'up', 'early', 'enough', 'for', 'it', '.']) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, t_pair in enumerate(t_pairs_lower):\n",
    "    print(str(index + 1) + \".\", t_pair, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And these are its similarities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 0.33333333333333337\n",
      "2. 0.4117647058823529\n",
      "3. 0.5714285714285714\n",
      "4. 0.4545454545454546\n",
      "5. 0.16666666666666663\n",
      "6. 0.13793103448275867\n"
     ]
    }
   ],
   "source": [
    "for index, similarity in enumerate(similarities_l_lower):\n",
    "    print(str(index + 1) + \".\", similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason behind this improvement is that there are tokens that are in both sentences but they differ in its first letter (as one is in uppercase and the other one is in lowercase). For example, in the third pair of sentences:\n",
    "\n",
    "- 'John said **he** is considered a witness but not a suspect.'\n",
    "- '\"**He** is not a suspect anymore.\" John said.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which is better: words or lemmas?\n",
    "\n",
    "The results show a better performance in lemmas tokenization than in words tokenization with a slightly increment of 0.1 in *Pearson correlation coefficient* as it is nearer to 1.0 (exact linear relationship). The reason of this increment is the second pair of sentences that was explained before (with the transformation of *invaded* to *invade*).\n",
    "\n",
    "The reason of this low correlation between the obtained results and the correct results, as it was explained in the last session, is that measuring similarities of a pair of sentences taking into consideration just the *Jaccard distance* (even with lemmas instead of words) is not enough to get the expected results.\n",
    "\n",
    "For example, the first pair of sentences (0.31 obtained vs 1.0 expected) means the same thing but are written using other words and consequently, the use of this technique might fail:\n",
    "\n",
    "- 'The bird **is** bathing **in the** sink.'\n",
    "- 'Birdie **is** washing itself **in the** water basin.'\n",
    "\n",
    "Other elements (for example, sentiment analysis or sentence context) should be studied to get a higher *Pearson correlation coefficient* between obtained results and expected results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do you think that could perform better for any pair of texts?\n",
    "\n",
    "The performance of lemmas tokenization is better than word tokenization only in the pair of texts that includes the appropiate morphemes. For example, as it was shown in the third pair of sentences:\n",
    "\n",
    "**Original Sentences**\n",
    "- 'In May 2010, the troops attempted to *invade* Kabul.'\n",
    "- 'The US army *invaded* Kabul on May 7th last year, 2010.'\n",
    "\n",
    "**Sentences tokenized by lemmas**\n",
    "- 'In may 2010, the troop attempt to **invade** kabul.'\n",
    "- 'The u army **invade** kabul on may 7th last year, 2010. '\n",
    "\n",
    "The performance is better as using the lemma token instead of the word token allows the system to compare correctly that both sentences have the idea of an \"invasion\".\n",
    "\n",
    "Nevertheless, the lemma tokenization does not perform better always as it can be seen in the following pair of sentences (first pair):\n",
    "\n",
    "**Original Sentences**\n",
    "- 'The bird **is** bathing in the sink.' \n",
    "- 'Birdie **is** washing itself in the water basin.'\n",
    "\n",
    "**Sentences tokenized by lemmas**\n",
    "- 'The bird **be** bath in the sink.'\n",
    "- 'birdie **be** wash itself in the water basin.'\n",
    "\n",
    "In this case, as it was already in the same tense, *Jaccard distance* computation will have the same result.\n",
    "\n",
    "Moreover, the morphemes of the words give information about the similarity of a pair of texts sometimes. For example:\n",
    "\n",
    "**Original Sentences Example**\n",
    "- 'He **went** to the supermarket'. \n",
    "- 'He **is going** to the supermarket'.\n",
    "\n",
    "When it is tokenized by words and lemmas results in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences tokenized by words: (['He', 'went', 'to', 'the', 'supermarket'], ['He', 'is', 'going', 'to', 'the', 'supermarket'])\n",
      "Sentences tokenized by lemmas: (['He', 'go', 'to', 'the', 'supermarket'], ['He', 'be', 'go', 'to', 'the', 'supermarket'])\n"
     ]
    }
   ],
   "source": [
    "new_pair_w = ((nltk.word_tokenize('He went to the supermarket'), \n",
    "               nltk.word_tokenize('He is going to the supermarket')))\n",
    "\n",
    "new_pair_l = ([lemmatize(word) for word in pos_tag(['He', 'went', 'to', 'the', 'supermarket'])],\n",
    "             [lemmatize(word) for word in pos_tag(['He', 'is', 'going', 'to', 'the', 'supermarket'])])\n",
    "\n",
    "print(\"Sentences tokenized by words:\", new_pair_w)\n",
    "print(\"Sentences tokenized by lemmas:\", new_pair_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between sentences tokenized by words: 0.5714285714285714\n",
      "Similarity between sentences tokenized by lemmas: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "print(\"Similarity between sentences tokenized by words:\", \n",
    "     (1 - jaccard_distance(set(new_pair_w[0]), set(new_pair_w[1]))))\n",
    "\n",
    "print(\"Similarity between sentences tokenized by lemmas:\", \n",
    "     (1 - jaccard_distance(set(new_pair_l[0]), set(new_pair_l[1]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the similarity between sentences tokenized by words is more accurate because the meaning of the sentences changes when a tokenization by lemmas is computed. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
