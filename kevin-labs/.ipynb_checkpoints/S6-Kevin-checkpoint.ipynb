{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# IHLT - Lab 6\n",
    "\n",
    "1. Read all pairs of sentences of the trial set within the evaluation framework of the project.\n",
    "\n",
    "2. Apply Leskâ€™s algorithm to the words in the sentences.\n",
    "\n",
    "3. Compute their similarities by considering senses and Jaccard coefficient.\n",
    "\n",
    "4. Compare the results with those in session 2 (document) and 3 (morphology) in which words and lemmas were considered.\n",
    "\n",
    "5. Compare the results with gold standard by giving the pearson correlation between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.wsd import lesk\n",
    "from nltk.metrics import jaccard_distance\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "As it was done in other lab sessions, all pairs of sentences of the trial set are read and stored in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. ('The bird is bathing in the sink.', 'Birdie is washing itself in the water basin.')\n",
      "2. ('In May 2010, the troops attempted to invade Kabul.', 'The US army invaded Kabul on May 7th last year, 2010.')\n",
      "3. ('John said he is considered a witness but not a suspect.', '\"He is not a suspect anymore.\" John said.')\n",
      "4. ('They flew out of the nest in groups.', 'They flew into the nest together.')\n",
      "5. ('The woman is playing the violin.', 'The young lady enjoys listening to the guitar.')\n",
      "6. ('John went horse back riding at dawn with a whole group of friends.', 'Sunrise at dawn is a magnificent view to take in if you wake up early enough for it.')\n"
     ]
    }
   ],
   "source": [
    "pairs = list()\n",
    "with open('trial/STS.input.txt','r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = nltk.TabTokenizer().tokenize(line.strip())\n",
    "        pairs.append((line[1], line[2]))\n",
    "        \n",
    "for index, pair in enumerate(pairs):\n",
    "    print(str(index + 1) + \".\", pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, each sentence is separated by its words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. (['The', 'bird', 'is', 'bathing', 'in', 'the', 'sink', '.'], ['Birdie', 'is', 'washing', 'itself', 'in', 'the', 'water', 'basin', '.']) \n",
      "\n",
      "2. (['In', 'May', '2010', ',', 'the', 'troops', 'attempted', 'to', 'invade', 'Kabul', '.'], ['The', 'US', 'army', 'invaded', 'Kabul', 'on', 'May', '7th', 'last', 'year', ',', '2010', '.']) \n",
      "\n",
      "3. (['John', 'said', 'he', 'is', 'considered', 'a', 'witness', 'but', 'not', 'a', 'suspect', '.'], ['``', 'He', 'is', 'not', 'a', 'suspect', 'anymore', '.', \"''\", 'John', 'said', '.']) \n",
      "\n",
      "4. (['They', 'flew', 'out', 'of', 'the', 'nest', 'in', 'groups', '.'], ['They', 'flew', 'into', 'the', 'nest', 'together', '.']) \n",
      "\n",
      "5. (['The', 'woman', 'is', 'playing', 'the', 'violin', '.'], ['The', 'young', 'lady', 'enjoys', 'listening', 'to', 'the', 'guitar', '.']) \n",
      "\n",
      "6. (['John', 'went', 'horse', 'back', 'riding', 'at', 'dawn', 'with', 'a', 'whole', 'group', 'of', 'friends', '.'], ['Sunrise', 'at', 'dawn', 'is', 'a', 'magnificent', 'view', 'to', 'take', 'in', 'if', 'you', 'wake', 'up', 'early', 'enough', 'for', 'it', '.']) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pairs = [(nltk.word_tokenize(p[0]), nltk.word_tokenize(p[1])) for p in pairs]\n",
    "\n",
    "for index, pair in enumerate(pairs):\n",
    "    print(str(index + 1) + \".\", pair, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the performance of the Lesk's Algorithm is improved if the PoS is inserted, it will be computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. ([('The', 'DT'), ('bird', 'NN'), ('is', 'VBZ'), ('bathing', 'VBG'), ('in', 'IN'), ('the', 'DT'), ('sink', 'NN'), ('.', '.')], [('Birdie', 'NNP'), ('is', 'VBZ'), ('washing', 'VBG'), ('itself', 'PRP'), ('in', 'IN'), ('the', 'DT'), ('water', 'NN'), ('basin', 'NN'), ('.', '.')]) \n",
      "\n",
      "2. ([('In', 'IN'), ('May', 'NNP'), ('2010', 'CD'), (',', ','), ('the', 'DT'), ('troops', 'NNS'), ('attempted', 'VBD'), ('to', 'TO'), ('invade', 'VB'), ('Kabul', 'NNP'), ('.', '.')], [('The', 'DT'), ('US', 'NNP'), ('army', 'NN'), ('invaded', 'VBD'), ('Kabul', 'NNP'), ('on', 'IN'), ('May', 'NNP'), ('7th', 'CD'), ('last', 'JJ'), ('year', 'NN'), (',', ','), ('2010', 'CD'), ('.', '.')]) \n",
      "\n",
      "3. ([('John', 'NNP'), ('said', 'VBD'), ('he', 'PRP'), ('is', 'VBZ'), ('considered', 'VBN'), ('a', 'DT'), ('witness', 'NN'), ('but', 'CC'), ('not', 'RB'), ('a', 'DT'), ('suspect', 'NN'), ('.', '.')], [('``', '``'), ('He', 'PRP'), ('is', 'VBZ'), ('not', 'RB'), ('a', 'DT'), ('suspect', 'NN'), ('anymore', 'RB'), ('.', '.'), (\"''\", \"''\"), ('John', 'NNP'), ('said', 'VBD'), ('.', '.')]) \n",
      "\n",
      "4. ([('They', 'PRP'), ('flew', 'VBD'), ('out', 'IN'), ('of', 'IN'), ('the', 'DT'), ('nest', 'JJS'), ('in', 'IN'), ('groups', 'NNS'), ('.', '.')], [('They', 'PRP'), ('flew', 'VBD'), ('into', 'IN'), ('the', 'DT'), ('nest', 'JJS'), ('together', 'RB'), ('.', '.')]) \n",
      "\n",
      "5. ([('The', 'DT'), ('woman', 'NN'), ('is', 'VBZ'), ('playing', 'VBG'), ('the', 'DT'), ('violin', 'NN'), ('.', '.')], [('The', 'DT'), ('young', 'JJ'), ('lady', 'NN'), ('enjoys', 'VBZ'), ('listening', 'VBG'), ('to', 'TO'), ('the', 'DT'), ('guitar', 'NN'), ('.', '.')]) \n",
      "\n",
      "6. ([('John', 'NNP'), ('went', 'VBD'), ('horse', 'NN'), ('back', 'RB'), ('riding', 'VBG'), ('at', 'IN'), ('dawn', 'NN'), ('with', 'IN'), ('a', 'DT'), ('whole', 'JJ'), ('group', 'NN'), ('of', 'IN'), ('friends', 'NNS'), ('.', '.')], [('Sunrise', 'NN'), ('at', 'IN'), ('dawn', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('magnificent', 'JJ'), ('view', 'NN'), ('to', 'TO'), ('take', 'VB'), ('in', 'IN'), ('if', 'IN'), ('you', 'PRP'), ('wake', 'VBP'), ('up', 'RP'), ('early', 'RB'), ('enough', 'RB'), ('for', 'IN'), ('it', 'PRP'), ('.', '.')]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pairs_pos = [(pos_tag(p[0]), pos_tag(p[1])) for p in pairs]\n",
    "\n",
    "for index, pair in enumerate(pairs_pos, 1):\n",
    "    print(str(index) + \".\", pair, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lesk's Algorithm\n",
    "`lesk(context_sentence, ambiguous_word, pos=None, ...)` from `nltk` performs the classic Lesk's algorithm for Word Sense Disambiguation (WSD) using the definitions of the ambiguous word. Lesk's algorithm uses the following formula:\n",
    "\n",
    "$ Lesk(w) = argmax_{s_i \\in S(\\{w\\})} \\forall _{s_j \\in S(C(w))} |Def(s_i) \\cap Def(s_j)| $\n",
    "\n",
    "Where:\n",
    "\n",
    "- $S(x)$ is the set of senses for all lemmas in X.\n",
    "- $C(w)$ is the set of lemmas in the context of word w.\n",
    "- $Def(s)$ is the set of lemmas in the definition of sense s.\n",
    "\n",
    "If `lesk(context_sentence, ambiguous_word, pos=None, ...)` returns `None` (PoS is not an Open-Class word or no synsets available), it is removed from the pairs list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. ([Synset('bird.n.02'), Synset('be.v.12'), Synset('bathe.v.01'), Synset('sinkhole.n.01')], [Synset('shuttlecock.n.01'), Synset('be.v.12'), Synset('wash.v.09'), Synset('body_of_water.n.01'), Synset('washbasin.n.01')]) \n",
      "\n",
      "2. ([Synset('whitethorn.n.01'), Synset('troop.n.02'), Synset('undertake.v.01'), Synset('invade.v.01'), Synset('kabul.n.01')], [Synset('uranium.n.01'), Synset('united_states_army.n.01'), Synset('invade.v.03'), Synset('kabul.n.01'), Synset('whitethorn.n.01'), Synset('last.a.02'), Synset('year.n.02')]) \n",
      "\n",
      "3. ([Synset('whoremaster.n.01'), Synset('suppose.v.01'), Synset('embody.v.02'), Synset('view.v.02'), Synset('witness.n.05'), Synset('not.r.01'), Synset('defendant.n.01')], [Synset('embody.v.02'), Synset('not.r.01'), Synset('defendant.n.01'), Synset('anymore.r.01'), Synset('whoremaster.n.01'), Synset('suppose.v.01')]) \n",
      "\n",
      "4. ([Synset('fly.v.12'), Synset('group.n.02')], [Synset('fly.v.10'), Synset('together.r.04')]) \n",
      "\n",
      "5. ([Synset('woman.n.02'), Synset('be.v.01'), Synset('play.v.35'), Synset('violin.n.01')], [Synset('young.a.01'), Synset('lady.n.03'), Synset('love.v.02'), Synset('heed.v.01'), Synset('guitar.n.01')]) \n",
      "\n",
      "6. ([Synset('toilet.n.01'), Synset('plump.v.04'), Synset('knight.n.02'), Synset('back.r.02'), Synset('ride.v.13'), Synset('dawn.n.01'), Synset('whole.a.02'), Synset('group.n.02'), Synset('friend.n.05')], [Synset('sunrise.n.03'), Synset('dawn.n.03'), Synset('be.v.12'), Synset('view.n.07'), Synset('take.v.34'), Synset('awaken.v.01'), Synset('up.r.05'), Synset('early_on.r.01'), Synset('enough.r.01')]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# If PoS is adjective ('j'), then return 'a' as PoS.\n",
    "def get_correct_pos(pos):\n",
    "    if pos == 'j':\n",
    "        return 'a'\n",
    "    return pos\n",
    "\n",
    "pairs_lesk = list()\n",
    "for index, pair in enumerate(pairs):\n",
    "    \n",
    "    first_sentence_lesk = [lesk(pair[0], word[0], pos=get_correct_pos(word[1][0].lower())) for word in pairs_pos[index][0]]\n",
    "    second_sentence_lesk = [lesk(pair[1], word[0], pos=get_correct_pos(word[1][0].lower())) for word in pairs_pos[index][1]]\n",
    "    \n",
    "    pairs_lesk.append(([synset for synset in first_sentence_lesk if synset is not None],\n",
    "                       [synset for synset in second_sentence_lesk if synset is not None]))\n",
    "    \n",
    "for index, pair in enumerate(pairs_lesk, 1):\n",
    "    print(str(index) + \".\", pair, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Similarities with Senses and *Jaccard Coefficient*\n",
    "\n",
    "As we have done in other lab sessions, the *Jaccard Coefficient* is used in order to compute the similarity between pairs of sentences. In this case, their senses are used instead of words or lemmas. Consequently, in this comparison, the similarities will be analyzed using the synsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities (considering senses):\n",
      "\n",
      "1. 0.125\n",
      "2. 0.19999999999999996\n",
      "3. 0.625\n",
      "4. 0.0\n",
      "5. 0.0\n",
      "6. 0.0\n"
     ]
    }
   ],
   "source": [
    "similarities = [1 - jaccard_distance(set(p[0]), set(p[1])) for p in pairs_lesk]\n",
    "\n",
    "print(\"Similarities (considering senses):\\n\")\n",
    "for index, similarity in enumerate(similarities, 1):\n",
    "    print(str(index) + \".\", similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparison with the previous results (words & lemmas)\n",
    "In order to analyze the comparison with the previous results (in session 2 and 3), those similarities will be shown:\n",
    "\n",
    "1) Similarities using words\n",
    "```\n",
    "1. 0.3076923076923077\n",
    "2. 0.26315789473684215\n",
    "3. 0.4666666666666667\n",
    "4. 0.4545454545454546\n",
    "5. 0.23076923076923073\n",
    "6. 0.13793103448275867\n",
    "```\n",
    "\n",
    "2) Similarities using lemmas\n",
    "```\n",
    "1. 0.3076923076923077\n",
    "2. 0.33333333333333337\n",
    "3. 0.4666666666666667\n",
    "4. 0.4545454545454546\n",
    "5. 0.23076923076923073\n",
    "6. 0.13793103448275867\n",
    "```\n",
    "\n",
    "In these similarities, the second pair of sentences was able to improve its performance as the word `invaded` was transformed to `invade` due to the lemma tokenization (second pair). Consequently, the similarities using lemmas improved the *Pearson Correlation Coefficient* from 0.39  (similarities using words) to 0.49 (similarities using lemmas).\n",
    "\n",
    "Nevertheless, in this session example, where the senses are being used (*i.e.* synsets), the results get very different. The underlying reason behind this is the poor existence of matching between exact meanings in the pairs of sentences, resulting in 3 pairs with no equivalence or similarity (pairs 4, 5 and 6), a pair with good similarity (3) and 2 pairs that are supposed to have a good similarity but get a small similarity (pairs 1 and 2).\n",
    "\n",
    "For example, the fourth pair of sentences can be studied:\n",
    "\n",
    "1. *They flew out of the nest in groups.*\n",
    "2. *They flew into the nest together.*\n",
    "\n",
    "Whose transformation into synsets taking into account the Lesk's algorithm is the following:\n",
    "\n",
    "1. *[Synset('fly.v.12'), Synset('group.n.02')]*\n",
    "2. *[Synset('fly.v.10'), Synset('together.r.04')]*\n",
    "\n",
    "In this case, word *flew* or lemma *fly* could support the similarity score. Nevertheless, in sense disambiguation, the synset is different.\n",
    "\n",
    "A comparison of their definition is inserted below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'travel over (an area of land or sea) in an aircraft'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_lesk[3][0][0].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'display in the air or cause to float'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_lesk[3][1][0].definition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the original sentences are analyzed, it is clear that the meaning is a bit different (fly out of the nest vs fly into the nest). However, a similarity score of `0` is too low for this example.\n",
    "\n",
    "The same situation happens with *groups vs. together*.\n",
    "\n",
    "If the second pair of sentences (with higher score using senses) is analyzed, the following *synsets* are proposed from these original sentences:\n",
    "\n",
    "1. *In May 2010, the troops attempted to invade Kabul.*\n",
    "2. *The US army invaded Kabul on May 7th last year, 2010.*\n",
    "\n",
    "\n",
    "1. *[**Synset('whitethorn.n.01')**, Synset('troop.n.02'), Synset('undertake.v.01'), Synset('invade.v.01'), **Synset('kabul.n.01')**]*.\n",
    "2. *[Synset('uranium.n.01'), Synset('united_states_army.n.01'), Synset('invade.v.03'), **Synset('kabul.n.01')**, **Synset('whitethorn.n.01')**, Synset('last.a.02'), Synset('year.n.02')]* \n",
    "\n",
    "As it can be seen, the synsets *kabul.n.01* and *Synset('whitethorn.n.01')* have the same sense in both sentences, increasing the similarity in this pair (*invade* has not got the same meaning in both sentences). Nevertheless, a similarity score of `0.20` is lower than expected in this case too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5. Comparison to the Gold Standard\n",
    "\n",
    "Finally, as it has been done in the other compared lab sessions, the similarities will be compared to the Gold Standard using the *Pearson Correlation Coefficient*.\n",
    "\n",
    "According to `00-readme.txt`, the similarity between two sentences that are completely equivalent should be 1/1 (or 5/5) and the similarity between two sentences that are on different topics should be 0/1 (or 0/5). For that reason, the reference similarities should be [1.0, 0.8, 0.6, 0.4, 0.2, 0] or its proportional values [5, 4, 3, 2, 1, 0].\n",
    "\n",
    "The values in the gold standard file STS.gs.txt are reversed, so they will be read and then inverted in order to get them correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold standard: [5, 4, 3, 2, 1, 0]\n",
      "Pearson correlation (sense): 0.40653613588298887\n"
     ]
    }
   ],
   "source": [
    "gs = list()\n",
    "with open('trial/STS.gs.txt','r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = nltk.TabTokenizer().tokenize(line.strip())\n",
    "        gs.append(int(line[1]))\n",
    "\n",
    "gs.reverse()\n",
    "print(\"Gold standard:\", gs)\n",
    "print(\"Pearson correlation (sense):\", pearsonr(gs, similarities)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this *Pearson correlation coefficient* is compared to the ones obtained in the previous sessions:\n",
    "\n",
    "```\n",
    "Pearson correlation (words): 0.3962389776119232\n",
    "Pearson correlation (lemmas): 0.490670810375692\n",
    "```\n",
    "\n",
    "It is supposed that, in this case, comparing the similarities between pairs of sentences using the word sense is better than comparing them using just words and worse than comparing them utilizing lemmas.\n",
    "\n",
    "Nevertheless, it must be noticed that the resulting similarities in this lab session are unstable. As it has been explained before, sometimes the similarity is `0` (fourth pair of sentences) since the words have a minimal difference in their sense. Moreover, Lesk's algorithm is not able occasionally to find any similar lemma in the definition of the senses in the context of the original word."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
