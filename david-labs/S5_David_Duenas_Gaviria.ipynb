{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 5 -  Lexical semantics\n",
    " \n",
    " Statement:\n",
    "  \n",
    "  * Given the following (lemma, category) pairs:\n",
    "  \n",
    "  (’the’,’DT’), (’man’,’NN’), (’swim’,’VB’), (’with’, ’PR’), (’a’, ’DT’),\n",
    "    (’girl’,’NN’), (’and’, ’CC’), (’a’, ’DT’), (’boy’, ’NN’), (’whilst’, ’PR’),\n",
    "    (’the’, ’DT’), (’woman’, ’NN’), (’walk’, ’VB’)\n",
    "    \n",
    " \n",
    "  * For each pair, when possible, print their most frequent WordNet synset, their corresponding least common subsumer (LCS) and their similarity value, using the following functions:\n",
    "  \n",
    "  \n",
    "     * Path Similarity\n",
    "     * Leacock-Chodorow Similarity\n",
    "     * Wu-Palmer Similarity\n",
    "     * Lin Similarity\n",
    "     \n",
    "   Normalize similarity values when necessary. What similarity seems better?\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import wordnet_ic\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define lemmas in a list\n",
    "lemmas_list=[('the', 'DT'),('man', 'NN'),('swim', 'VB'),('the', 'DT'),('with', 'PR'),('a', 'DT'),('girl', 'NN'),('and', 'CC'),\n",
    " ('a', 'DT'),('boy', 'NN'),('whilst', 'PR'),('the', 'DT'),('woman', 'NN'),('walk', 'VB')]\n",
    "\n",
    "len(lemmas_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most frequent synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('man.n.01'),\n",
       " Synset('swim.v.01'),\n",
       " Synset('girl.n.01'),\n",
       " Synset('male_child.n.01'),\n",
       " Synset('woman.n.01'),\n",
       " Synset('walk.v.01')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most frequent synset with his respective POS\n",
    "#Placeholder for synset list\n",
    "synset_list=[]\n",
    "# Loop the list to extract the lemmas\n",
    "for i in range(len(lemmas_list)):\n",
    "    # avoid 'a' since it was identified as vitamin\n",
    "    if (lemmas_list[i][0]!='a'):\n",
    "        # Get synset with Verb from lemma\n",
    "        if(lemmas_list[i][1]=='VB'):\n",
    "            # as it can take man as a verb\n",
    "            if(lemmas_list[i][0]!='man'):\n",
    "                synset = wn.synsets(lemmas_list[i][0],'v')\n",
    "                # Add the one with most frequence to list if is not blank space\n",
    "                if (synset!=[]):\n",
    "                    synset_list.append(synset[0])\n",
    "        # Get synset with Noun from lemma\n",
    "        if(lemmas_list[i][1]=='NN'):\n",
    "            synset = wn.synsets(lemmas_list[i][0],'n')\n",
    "            # Add the one with most frequence to list if is not blank space\n",
    "            if (synset!=[]):\n",
    "                synset_list.append(synset[0])\n",
    "\n",
    "# Cleaned list of synsets       \n",
    "synset_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least common subsumer (LCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Synset('man.n.01')],\n",
       " [Synset('adult.n.01')],\n",
       " [Synset('male.n.02')],\n",
       " [Synset('swim.v.01')],\n",
       " [Synset('travel.v.01')],\n",
       " [Synset('girl.n.01')],\n",
       " [Synset('person.n.01')],\n",
       " [Synset('woman.n.01')],\n",
       " [Synset('male_child.n.01')],\n",
       " [Synset('walk.v.01')]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Least common subsumer matrix\n",
    "# Define empty LCS\n",
    "LCS=[]\n",
    "matrix_row=[]\n",
    "# Loop to compute the LCS\n",
    "for i in range(len(synset_list)):\n",
    "    for j in range(len(synset_list)):\n",
    "        # get LCS for each other\n",
    "        temp = synset_list[i].lowest_common_hypernyms(synset_list[j])  \n",
    "        # check if ancestor is not repeated\n",
    "        if(temp not in LCS and temp!=[]):\n",
    "            LCS.append(temp)\n",
    "LCS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0],\n",
       " [None],\n",
       " [0.631578947368421],\n",
       " [0.6666666666666666],\n",
       " [0.6666666666666666],\n",
       " [None],\n",
       " [0.18181818181818182],\n",
       " [1.0],\n",
       " [0.16666666666666666],\n",
       " [0.18181818181818182],\n",
       " [0.18181818181818182],\n",
       " [0.3333333333333333],\n",
       " [0.631578947368421],\n",
       " [None],\n",
       " [1.0],\n",
       " [0.631578947368421],\n",
       " [0.631578947368421],\n",
       " [None],\n",
       " [0.6666666666666666],\n",
       " [None],\n",
       " [0.631578947368421],\n",
       " [1.0],\n",
       " [0.6666666666666666],\n",
       " [None],\n",
       " [0.6666666666666666],\n",
       " [None],\n",
       " [0.9473684210526315],\n",
       " [0.6666666666666666],\n",
       " [1.0],\n",
       " [None],\n",
       " [0.18181818181818182],\n",
       " [0.3333333333333333],\n",
       " [0.16666666666666666],\n",
       " [0.18181818181818182],\n",
       " [0.18181818181818182],\n",
       " [1.0]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wup matrix\n",
    "wup=[]\n",
    "matrix_row=[]\n",
    "# Loop to compute the wup matrix\n",
    "for i in range(len(synset_list)):\n",
    "    for j in range(len(synset_list)):\n",
    "        temp = synset_list[i].wup_similarity(synset_list[j])  \n",
    "        matrix_row.append(temp)\n",
    "        # add row\n",
    "        wup.append(matrix_row)\n",
    "        # refresh matrix row\n",
    "        matrix_row=[]\n",
    "wup\n",
    "# none can be interpret as zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0],\n",
       " [None],\n",
       " [0.25],\n",
       " [0.3333333333333333],\n",
       " [0.3333333333333333],\n",
       " [None],\n",
       " [0.1],\n",
       " [1.0],\n",
       " [0.09090909090909091],\n",
       " [0.1],\n",
       " [0.1],\n",
       " [0.3333333333333333],\n",
       " [0.25],\n",
       " [None],\n",
       " [1.0],\n",
       " [0.16666666666666666],\n",
       " [0.5],\n",
       " [None],\n",
       " [0.3333333333333333],\n",
       " [None],\n",
       " [0.16666666666666666],\n",
       " [1.0],\n",
       " [0.2],\n",
       " [None],\n",
       " [0.3333333333333333],\n",
       " [None],\n",
       " [0.5],\n",
       " [0.2],\n",
       " [1.0],\n",
       " [None],\n",
       " [0.1],\n",
       " [0.3333333333333333],\n",
       " [0.09090909090909091],\n",
       " [0.1],\n",
       " [0.1],\n",
       " [1.0]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path similarity\n",
    "path=[]\n",
    "matrix_row=[]\n",
    "# Loop to compute the wup matrix\n",
    "for i in range(len(synset_list)):\n",
    "    for j in range(len(synset_list)):\n",
    "        temp = synset_list[i].path_similarity(synset_list[j])  \n",
    "        matrix_row.append(temp)\n",
    "        # add row\n",
    "        path.append(matrix_row)\n",
    "        # refresh matrix row\n",
    "        matrix_row=[]\n",
    "path\n",
    "# none can be interpret as zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0],\n",
       " [1.0],\n",
       " [0.6188971751464533],\n",
       " [0.6979831568441128],\n",
       " [0.6979831568441128],\n",
       " [0.6979831568441128],\n",
       " [0.6979831568441128],\n",
       " [0.8956754273186898],\n",
       " [0.8956754273186898],\n",
       " [0.8956754273186898],\n",
       " [0.8956754273186898],\n",
       " [0.5936585841628025],\n",
       " [0.6188971751464533],\n",
       " [0.6188971751464533],\n",
       " [1.0],\n",
       " [0.5074317444173395],\n",
       " [0.8094485875732267],\n",
       " [0.8094485875732267],\n",
       " [0.6979831568441128],\n",
       " [0.6979831568441128],\n",
       " [0.5074317444173395],\n",
       " [1.0],\n",
       " [0.5575533219658061],\n",
       " [0.5575533219658061],\n",
       " [0.6979831568441128],\n",
       " [0.6979831568441128],\n",
       " [0.8094485875732267],\n",
       " [0.5575533219658061],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [0.5936585841628025],\n",
       " [0.5936585841628025],\n",
       " [0.5936585841628025],\n",
       " [0.5936585841628025],\n",
       " [0.8956754273186898]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Leacock-Chodorow similarity\n",
    "lch=[]\n",
    "matrix_row=[]\n",
    "POS_issues=0\n",
    "\n",
    "# max lch for normaliaztion\n",
    "max_lch = synset_list[0].lch_similarity(synset_list[0])\n",
    "# Loop to compute the wup matrix\n",
    "for i in range(len(synset_list)):\n",
    "    for j in range(len(synset_list)):\n",
    "        # Handle if is not able to compute similarity because of the POS\n",
    "        try:\n",
    "            temp = synset_list[i].lch_similarity(synset_list[j])/max_lch  \n",
    "        except:\n",
    "            POS_issues=POS_issues+1\n",
    "        matrix_row.append(temp)\n",
    "        # add row\n",
    "        lch.append(matrix_row)\n",
    "        # refresh matrix row\n",
    "        matrix_row=[]\n",
    "lch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0],\n",
       " [1.0],\n",
       " [0.7135111237276783],\n",
       " [0.7294717876200584],\n",
       " [0.7870841372982784],\n",
       " [0.7870841372982784],\n",
       " [0.7870841372982784],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [0.4910052007916556],\n",
       " [0.7135111237276783],\n",
       " [0.7135111237276783],\n",
       " [1.0],\n",
       " [0.2927280671561499],\n",
       " [0.9067798595489287],\n",
       " [0.9067798595489287],\n",
       " [0.7294717876200584],\n",
       " [0.7294717876200584],\n",
       " [0.2927280671561499],\n",
       " [1.0],\n",
       " [0.31842335630818425],\n",
       " [0.31842335630818425],\n",
       " [0.7870841372982784],\n",
       " [0.7870841372982784],\n",
       " [0.9067798595489287],\n",
       " [0.31842335630818425],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [1.0],\n",
       " [0.4910052007916556],\n",
       " [0.4910052007916556],\n",
       " [0.4910052007916556],\n",
       " [0.4910052007916556],\n",
       " [1.0]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lin similarity matrix\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "\n",
    "POS_issues=0\n",
    "lin=[]\n",
    "matrix_row=[]\n",
    "# Loop to compute the wup matrix\n",
    "for i in range(len(synset_list)):\n",
    "    for j in range(len(synset_list)):\n",
    "        # Handle if is not able to compute similarity because of the POS\n",
    "        try:\n",
    "            temp = synset_list[i].lin_similarity(synset_list[j],brown_ic)  \n",
    "        except:\n",
    "            POS_issues=POS_issues+1\n",
    "        matrix_row.append(temp)\n",
    "        # add row\n",
    "        lin.append(matrix_row)\n",
    "        # refresh matrix row\n",
    "        matrix_row=[]\n",
    "lin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is, if I want how similar two word senses are, having into account the Least Common Subsumer of the two input Synsets I'll choose the Lin similarity, or the wu palmer, that in most cases showed similar results. But if I want a plain similarity based on the shortest path that connects the senses in his taxonomy, the path similarity does the job.\n",
    "\n",
    "The matrices wont have the same dimensions since there are similarities that won't allow the computation, but this doesn't necesarilly means a better similarity method. As it'll show a similarity between two different POS for the synset, and this naturally won't be a desired for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
